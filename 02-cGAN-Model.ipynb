{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow_privacy.privacy.optimizers import dp_optimizer_vectorized\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.analysis.compute_noise_from_budget_lib import compute_noise\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "from synthesizers.cgan.model import (\n",
    "    ConditionalGAN, \n",
    "    GANMonitor\n",
    ")\n",
    "from synthesizers.preprocessing.wesad import (\n",
    "    WESADDataset, \n",
    "    LabelType\n",
    ")\n",
    "from synthesizers.utils.training import data_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 1\n",
    "USE_SLIDING_WINDOWS = True\n",
    "\n",
    "# Training Hyperparameters\n",
    "DP_TRAINING = True\n",
    "NUM_FEATURES = 6\n",
    "SEQ_LENGTH = 60\n",
    "LATENT_DIM = SEQ_LENGTH\n",
    "BATCH_SIZE = 8\n",
    "HIDDEN_UNITS = 64\n",
    "EPOCHS = 420\n",
    "ACTIVATION = \"relu\"\n",
    "RANDOM_SEED = 42\n",
    "LEARNING_RATE = 0.0002\n",
    "LOSS_FN = \"binary_cross_entropy\"\n",
    "D_ARCHITECTURE = \"lstm\"\n",
    "LOSO_TRAINING_WITHOUT_SUBJECT = False # \"14\"\n",
    "\n",
    "# DP Training Hyperparameter\n",
    "L2_NORM_CLIP = 1.0\n",
    "NUM_MICROBATCHES = BATCH_SIZE\n",
    "DP_LEARNING_RATE = 1e-3\n",
    "DELTA = 1e-4\n",
    "\n",
    "\n",
    "# Define run config\n",
    "config = {\n",
    "    \"activation_function\": ACTIVATION,\n",
    "    \"hidden_units\": HIDDEN_UNITS,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"num_features\": NUM_FEATURES,\n",
    "    \"seq_length\": SEQ_LENGTH,\n",
    "    \"dp_training\": DP_TRAINING,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"loss_function\": LOSS_FN,\n",
    "    \"d_architecture\": D_ARCHITECTURE,\n",
    "    \"use_sliding_windows\": USE_SLIDING_WINDOWS\n",
    "}\n",
    "\n",
    "if LOSO_TRAINING_WITHOUT_SUBJECT:\n",
    "    config[\"WESAD_WITHOUT_SUBJ\"] = LOSO_TRAINING_WITHOUT_SUBJECT\n",
    "\n",
    "if DP_TRAINING:\n",
    "    config[\"l2_norm_clip\"] = L2_NORM_CLIP\n",
    "    config[\"num_microbatches\"] = NUM_MICROBATCHES\n",
    "    config[\"dp_learning_rate\"] = DP_LEARNING_RATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and split it into stress and non-stress for later testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 60, 6)\n",
      "(64, 60, 6)\n",
      "(610, 60, 6)\n",
      "(153, 60, 6)\n"
     ]
    }
   ],
   "source": [
    "windows = np.load('data/wesad/wesad_windows.npy')\n",
    "labels = np.load('data/wesad/wesad_labels.npy')\n",
    "\n",
    "if USE_SLIDING_WINDOWS:\n",
    "    mos = windows[labels == 1]\n",
    "    non_mos = windows[labels == 0]\n",
    "else:\n",
    "    mos = windows[labels == 1]\n",
    "    non_mos = windows[labels == 0]\n",
    "\n",
    "windows = np.delete(windows, 6, axis=2)\n",
    "mos = np.delete(mos, 6, axis=2)\n",
    "non_mos = np.delete(non_mos, 6, axis=2)\n",
    "\n",
    "num_split = 0.8\n",
    "trainmos, testmos = data_split(mos, num_split)\n",
    "trainnomos, testnomos = data_split(non_mos, num_split)\n",
    "\n",
    "print(trainmos.shape)\n",
    "print(testmos.shape)\n",
    "print(trainnomos.shape)\n",
    "print(testnomos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the noise for privacy guarantees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 1.48% and noise_multiplier = 103.99275642585638 iterated over 56805 steps satisfies differential privacy with eps = 0.1 and delta = 0.0001.\n",
      "DP-SGD with sampling rate = 1.48% and noise_multiplier = 12.393045830417874 iterated over 56805 steps satisfies differential privacy with eps = 1 and delta = 0.0001.\n",
      "DP-SGD with sampling rate = 1.48% and noise_multiplier = 1.8509817962014676 iterated over 56805 steps satisfies differential privacy with eps = 10 and delta = 0.0001.\n",
      "{0.1: 103.99275642585638, 1: 12.393045830417874, 10: 1.8509817962014676}\n",
      "DP-SGD with sampling rate = 1.48% and noise_multiplier = 103.99275642585638 iterated over 56805 steps satisfies differential privacy with eps = 0.1 and delta = 0.0001.\n",
      "The optimal RDP order is 128.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09999999999999426, 128.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get needed noise for target epsilon\n",
    "min_noise = 1e-5\n",
    "target_epsilons = [0.1, 1, 10]\n",
    "noise_multipliers = {target_epsilon : compute_noise(\n",
    "    windows.shape[0] // 2,\n",
    "    BATCH_SIZE,\n",
    "    target_epsilon,\n",
    "    EPOCHS * 2,\n",
    "    DELTA,\n",
    "    min_noise\n",
    ") for target_epsilon in target_epsilons}\n",
    "print(noise_multipliers)\n",
    "\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=windows.shape[0] // 2,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              noise_multiplier=noise_multipliers[target_epsilons[0]],\n",
    "                                              epochs=EPOCHS*2,\n",
    "                                              delta=DELTA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweep Config only for sweep case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create TF dataset from windows and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into tf dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((windows, labels))\n",
    "\n",
    "# Shuffle, cache, and batch the dataset\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "randomTrainMos = tf.random.normal(shape=(trainmos.shape[0], LATENT_DIM))\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "randomTrainNoMos = tf.random.normal(shape=(trainnomos.shape[0], LATENT_DIM))\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "randomTestMos = tf.random.normal(shape=(testmos.shape[0], LATENT_DIM))\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "randomTestNoMos = tf.random.normal(shape=(testnomos.shape[0], LATENT_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training in wandb environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_architecture:  lstm\n",
      "<tensorflow_privacy.privacy.optimizers.dp_optimizer_vectorized.make_vectorized_optimizer_class.<locals>.DPOptimizerClass object at 0x1785de9a0> is used\n",
      "Epoch 1/420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 13:30:28.066551: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 15s 59ms/step - g_loss: 0.3815 - d_loss: 0.6160 - div_term: 0.5817\n",
      "Epoch 2/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 0.5574 - d_loss: 0.5428 - div_term: 0.4824\n",
      "Epoch 3/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 3.9103 - d_loss: 0.0203 - div_term: 0.0306\n",
      "Epoch 4/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 5.8656 - d_loss: 0.0023 - div_term: 0.0103\n",
      "Epoch 5/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 6.5748 - d_loss: 0.0012 - div_term: 0.0110\n",
      "Epoch 6/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 7.0272 - d_loss: 7.8178e-04 - div_term: 0.0147\n",
      "Epoch 7/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 7.3700 - d_loss: 5.6598e-04 - div_term: 0.0208\n",
      "Epoch 8/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 7.6370 - d_loss: 4.3477e-04 - div_term: 0.0394\n",
      "Epoch 9/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 4.5398 - d_loss: 0.5276 - div_term: 0.2232\n",
      "Epoch 10/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 3.2491 - d_loss: 0.0594 - div_term: 0.0431\n",
      "Epoch 11/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 4.5889 - d_loss: 0.0067 - div_term: 0.0288\n",
      "Epoch 12/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 5.2413 - d_loss: 0.0052 - div_term: 0.0337\n",
      "Epoch 13/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 5.6437 - d_loss: 0.0220 - div_term: 0.0980\n",
      "Epoch 14/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 5.3368 - d_loss: 0.0058 - div_term: 0.0561\n",
      "Epoch 15/420\n",
      "136/136 [==============================] - 295s 2s/step - g_loss: 5.3611 - d_loss: 0.0200 - div_term: 0.2869\n",
      "Epoch 16/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: 5.1636 - d_loss: 0.0167 - div_term: 0.1971\n",
      "Epoch 17/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 5.4907 - d_loss: 0.0021 - div_term: 0.2091\n",
      "Epoch 18/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 5.8426 - d_loss: 0.0014 - div_term: 0.2922\n",
      "Epoch 19/420\n",
      "136/136 [==============================] - 1941s 14s/step - g_loss: 5.5022 - d_loss: 0.0017 - div_term: 0.0876\n",
      "Epoch 20/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 6.0192 - d_loss: 0.0014 - div_term: 0.0061\n",
      "Epoch 21/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 6.3521 - d_loss: 0.0010 - div_term: 0.0052\n",
      "Epoch 22/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 6.6084 - d_loss: 7.9515e-04 - div_term: 0.0053\n",
      "Epoch 23/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 6.8250 - d_loss: 6.4797e-04 - div_term: 0.0050\n",
      "Epoch 24/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 7.0120 - d_loss: 5.4207e-04 - div_term: 0.0048\n",
      "Epoch 25/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 7.1786 - d_loss: 4.6185e-04 - div_term: 0.0048\n",
      "Epoch 26/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 7.3299 - d_loss: 3.9906e-04 - div_term: 0.0045\n",
      "Epoch 27/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 7.4694 - d_loss: 3.4847e-04 - div_term: 0.0044\n",
      "Epoch 28/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 7.5994 - d_loss: 3.0696e-04 - div_term: 0.0044\n",
      "Epoch 29/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 7.7218 - d_loss: 2.7224e-04 - div_term: 0.0043\n",
      "Epoch 30/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 7.8378 - d_loss: 2.4280e-04 - div_term: 0.0046\n",
      "Epoch 31/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 7.9484 - d_loss: 2.1761e-04 - div_term: 0.0048\n",
      "Epoch 32/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 8.0552 - d_loss: 1.9579e-04 - div_term: 0.0050\n",
      "Epoch 33/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 8.1580 - d_loss: 1.7680e-04 - div_term: 0.0050\n",
      "Epoch 34/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 8.2579 - d_loss: 1.6009e-04 - div_term: 0.0052\n",
      "Epoch 35/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 8.3547 - d_loss: 1.4536e-04 - div_term: 0.0059\n",
      "Epoch 36/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 8.4490 - d_loss: 1.3228e-04 - div_term: 0.0065\n",
      "Epoch 37/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 8.5404 - d_loss: 1.2062e-04 - div_term: 0.0076\n",
      "Epoch 38/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 8.6297 - d_loss: 1.1025e-04 - div_term: 0.0098\n",
      "Epoch 39/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 8.7115 - d_loss: 1.8003e-04 - div_term: 0.0187\n",
      "Epoch 40/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 7.0903 - d_loss: 0.1017 - div_term: 0.0066\n",
      "Epoch 41/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 5.6990 - d_loss: 0.0038 - div_term: 0.0012\n",
      "Epoch 42/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 6.2363 - d_loss: 0.0016 - div_term: 0.0068\n",
      "Epoch 43/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 6.6206 - d_loss: 7.3192e-04 - div_term: 0.0145\n",
      "Epoch 44/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 7.0078 - d_loss: 5.0023e-04 - div_term: 0.0135\n",
      "Epoch 45/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: 7.3072 - d_loss: 3.7326e-04 - div_term: 0.0110\n",
      "Epoch 46/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 7.5558 - d_loss: 2.9277e-04 - div_term: 0.0096\n",
      "Epoch 47/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 7.7669 - d_loss: 2.3728e-04 - div_term: 0.0112\n",
      "Epoch 48/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 7.9606 - d_loss: 1.9630e-04 - div_term: 0.0107\n",
      "Epoch 49/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 8.1466 - d_loss: 1.6375e-04 - div_term: 0.0095\n",
      "Epoch 50/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 8.3399 - d_loss: 1.3620e-04 - div_term: 0.0075\n",
      "Epoch 51/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 8.5445 - d_loss: 1.1233e-04 - div_term: 0.0063\n",
      "Epoch 52/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 8.7352 - d_loss: 9.3955e-05 - div_term: 0.0048\n",
      "Epoch 53/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 8.8924 - d_loss: 8.0915e-05 - div_term: 0.0040\n",
      "Epoch 54/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: 9.0290 - d_loss: 7.1028e-05 - div_term: 0.0029\n",
      "Epoch 55/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 9.1521 - d_loss: 6.3055e-05 - div_term: 0.0024\n",
      "Epoch 56/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: 9.2668 - d_loss: 5.6393e-05 - div_term: 0.0021\n",
      "Epoch 57/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 9.3758 - d_loss: 5.0709e-05 - div_term: 0.0017\n",
      "Epoch 58/420\n",
      "136/136 [==============================] - 10s 76ms/step - g_loss: 9.4801 - d_loss: 4.5793e-05 - div_term: 0.0013\n",
      "Epoch 59/420\n",
      "136/136 [==============================] - 10s 76ms/step - g_loss: 9.5804 - d_loss: 4.1501e-05 - div_term: 0.0011\n",
      "Epoch 60/420\n",
      "136/136 [==============================] - 10s 76ms/step - g_loss: 9.6780 - d_loss: 3.7709e-05 - div_term: 9.6672e-04\n",
      "Epoch 61/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 9.7745 - d_loss: 3.4306e-05 - div_term: 8.7675e-04\n",
      "Epoch 62/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 9.8721 - d_loss: 3.1196e-05 - div_term: 6.5482e-04\n",
      "Epoch 63/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 9.9726 - d_loss: 2.8309e-05 - div_term: 5.9109e-04\n",
      "Epoch 64/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 10.0777 - d_loss: 2.5603e-05 - div_term: 5.1856e-04\n",
      "Epoch 65/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 10.1885 - d_loss: 2.3060e-05 - div_term: 4.8646e-04\n",
      "Epoch 66/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 10.3062 - d_loss: 2.0666e-05 - div_term: 4.6499e-04\n",
      "Epoch 67/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: 10.4349 - d_loss: 1.8376e-05 - div_term: 4.3692e-04\n",
      "Epoch 68/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 10.5779 - d_loss: 1.6172e-05 - div_term: 4.2961e-04\n",
      "Epoch 69/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 10.7254 - d_loss: 1.4202e-05 - div_term: 4.6504e-04\n",
      "Epoch 70/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 10.8565 - d_loss: 1.2647e-05 - div_term: 4.4268e-04\n",
      "Epoch 71/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 10.9637 - d_loss: 1.1475e-05 - div_term: 4.4697e-04\n",
      "Epoch 72/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 11.0534 - d_loss: 1.0552e-05 - div_term: 4.4351e-04\n",
      "Epoch 73/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 11.1324 - d_loss: 9.7794e-06 - div_term: 4.0190e-04\n",
      "Epoch 74/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 11.2050 - d_loss: 9.1044e-06 - div_term: 4.1103e-04\n",
      "Epoch 75/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 11.2739 - d_loss: 8.4984e-06 - div_term: 3.9424e-04\n",
      "Epoch 76/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 11.3403 - d_loss: 7.9454e-06 - div_term: 3.8610e-04\n",
      "Epoch 77/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 11.4053 - d_loss: 7.4357e-06 - div_term: 3.7785e-04\n",
      "Epoch 78/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 11.4694 - d_loss: 6.9629e-06 - div_term: 3.5325e-04\n",
      "Epoch 79/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 11.5328 - d_loss: 6.5227e-06 - div_term: 3.3974e-04\n",
      "Epoch 80/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 11.5959 - d_loss: 6.1116e-06 - div_term: 3.2968e-04\n",
      "Epoch 81/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 11.6588 - d_loss: 5.7271e-06 - div_term: 3.3009e-04\n",
      "Epoch 82/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 11.7217 - d_loss: 5.3670e-06 - div_term: 3.1708e-04\n",
      "Epoch 83/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 11.7845 - d_loss: 5.0293e-06 - div_term: 3.1435e-04\n",
      "Epoch 84/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 11.8475 - d_loss: 4.7127e-06 - div_term: 2.9921e-04\n",
      "Epoch 85/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 11.9106 - d_loss: 4.4155e-06 - div_term: 2.8962e-04\n",
      "Epoch 86/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 11.9738 - d_loss: 4.1366e-06 - div_term: 2.9328e-04\n",
      "Epoch 87/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 12.0372 - d_loss: 3.8748e-06 - div_term: 2.8854e-04\n",
      "Epoch 88/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 12.1009 - d_loss: 3.6291e-06 - div_term: 2.8893e-04\n",
      "Epoch 89/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 12.1647 - d_loss: 3.3983e-06 - div_term: 2.7567e-04\n",
      "Epoch 90/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 12.2288 - d_loss: 3.1817e-06 - div_term: 2.5867e-04\n",
      "Epoch 91/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 12.2932 - d_loss: 2.9784e-06 - div_term: 2.4982e-04\n",
      "Epoch 92/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 12.3578 - d_loss: 2.7876e-06 - div_term: 2.7778e-04\n",
      "Epoch 93/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 12.4227 - d_loss: 2.6086e-06 - div_term: 2.2906e-04\n",
      "Epoch 94/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 12.4877 - d_loss: 2.4407e-06 - div_term: 2.3306e-04\n",
      "Epoch 95/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 12.5530 - d_loss: 2.2832e-06 - div_term: 2.0507e-04\n",
      "Epoch 96/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 12.6185 - d_loss: 2.1356e-06 - div_term: 2.3108e-04\n",
      "Epoch 97/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 12.6843 - d_loss: 1.9972e-06 - div_term: 1.9396e-04\n",
      "Epoch 98/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 12.7501 - d_loss: 1.8676e-06 - div_term: 2.0023e-04\n",
      "Epoch 99/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 12.8162 - d_loss: 1.7462e-06 - div_term: 1.9915e-04\n",
      "Epoch 100/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 12.8823 - d_loss: 1.6325e-06 - div_term: 1.8351e-04\n",
      "Epoch 101/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 12.9486 - d_loss: 1.5261e-06 - div_term: 1.7864e-04\n",
      "Epoch 102/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 13.0150 - d_loss: 1.4265e-06 - div_term: 1.6954e-04\n",
      "Epoch 103/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 13.0815 - d_loss: 1.3334e-06 - div_term: 1.6595e-04\n",
      "Epoch 104/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 13.1481 - d_loss: 1.2463e-06 - div_term: 1.6806e-04\n",
      "Epoch 105/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 13.2147 - d_loss: 1.1648e-06 - div_term: 1.4910e-04\n",
      "Epoch 106/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 13.2814 - d_loss: 1.0886e-06 - div_term: 1.4890e-04\n",
      "Epoch 107/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 13.3481 - d_loss: 1.0174e-06 - div_term: 1.3964e-04\n",
      "Epoch 108/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 13.4149 - d_loss: 9.5075e-07 - div_term: 1.3712e-04\n",
      "Epoch 109/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 13.4818 - d_loss: 8.8842e-07 - div_term: 1.4603e-04\n",
      "Epoch 110/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 13.5487 - d_loss: 8.3020e-07 - div_term: 1.2569e-04\n",
      "Epoch 111/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 13.6158 - d_loss: 7.7561e-07 - div_term: 1.1576e-04\n",
      "Epoch 112/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 13.6828 - d_loss: 7.2470e-07 - div_term: 1.1457e-04\n",
      "Epoch 113/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 13.7500 - d_loss: 6.7711e-07 - div_term: 1.1394e-04\n",
      "Epoch 114/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: 13.8174 - d_loss: 6.3247e-07 - div_term: 1.1786e-04\n",
      "Epoch 115/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 13.8849 - d_loss: 5.9068e-07 - div_term: 1.0635e-04\n",
      "Epoch 116/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 13.9525 - d_loss: 5.5175e-07 - div_term: 9.4096e-05\n",
      "Epoch 117/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 14.0202 - d_loss: 5.1532e-07 - div_term: 9.4964e-05\n",
      "Epoch 118/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 14.0883 - d_loss: 4.8112e-07 - div_term: 1.0117e-04\n",
      "Epoch 119/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 14.1568 - d_loss: 4.4897e-07 - div_term: 9.6777e-05\n",
      "Epoch 120/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 14.2255 - d_loss: 4.1896e-07 - div_term: 8.7702e-05\n",
      "Epoch 121/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 14.2942 - d_loss: 3.9101e-07 - div_term: 9.1924e-05\n",
      "Epoch 122/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 14.3634 - d_loss: 3.6484e-07 - div_term: 8.5297e-05\n",
      "Epoch 123/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: 14.4330 - d_loss: 3.4030e-07 - div_term: 7.9289e-05\n",
      "Epoch 124/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: 14.5033 - d_loss: 3.1722e-07 - div_term: 7.5597e-05\n",
      "Epoch 125/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 14.5743 - d_loss: 2.9548e-07 - div_term: 8.2694e-05\n",
      "Epoch 126/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 14.6463 - d_loss: 2.7494e-07 - div_term: 7.3189e-05\n",
      "Epoch 127/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: 14.7188 - d_loss: 2.5575e-07 - div_term: 8.0871e-05\n",
      "Epoch 128/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 14.7913 - d_loss: 2.3802e-07 - div_term: 7.2949e-05\n",
      "Epoch 129/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 14.8640 - d_loss: 2.2158e-07 - div_term: 6.8080e-05\n",
      "Epoch 130/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 14.9367 - d_loss: 2.0631e-07 - div_term: 6.6811e-05\n",
      "Epoch 131/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 15.0093 - d_loss: 1.9217e-07 - div_term: 6.9161e-05\n",
      "Epoch 132/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 15.0816 - d_loss: 1.7905e-07 - div_term: 7.1860e-05\n",
      "Epoch 133/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 15.1537 - d_loss: 1.6683e-07 - div_term: 6.7502e-05\n",
      "Epoch 134/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: 15.2256 - d_loss: 1.5543e-07 - div_term: 7.1718e-05\n",
      "Epoch 135/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 15.2972 - d_loss: 1.4478e-07 - div_term: 7.8092e-05\n",
      "Epoch 136/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 15.3687 - d_loss: 1.3481e-07 - div_term: 7.6483e-05\n",
      "Epoch 137/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 15.4400 - d_loss: 1.2546e-07 - div_term: 8.2161e-05\n",
      "Epoch 138/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 15.5114 - d_loss: 1.1669e-07 - div_term: 8.6343e-05\n",
      "Epoch 139/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 15.5830 - d_loss: 1.0847e-07 - div_term: 9.3016e-05\n",
      "Epoch 140/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 15.6547 - d_loss: 1.0075e-07 - div_term: 9.7226e-05\n",
      "Epoch 141/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 15.7267 - d_loss: 9.3500e-08 - div_term: 1.0293e-04\n",
      "Epoch 142/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 15.7980 - d_loss: 8.6855e-08 - div_term: 1.0660e-04\n",
      "Epoch 143/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 15.8666 - d_loss: 8.1036e-08 - div_term: 1.0706e-04\n",
      "Epoch 144/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 15.9333 - d_loss: 7.5849e-08 - div_term: 1.1029e-04\n",
      "Epoch 145/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 15.9987 - d_loss: 7.1145e-08 - div_term: 1.0767e-04\n",
      "Epoch 146/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 16.0626 - d_loss: 6.6898e-08 - div_term: 1.1222e-04\n",
      "Epoch 147/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: 16.1255 - d_loss: 6.3013e-08 - div_term: 1.0861e-04\n",
      "Epoch 148/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 16.1877 - d_loss: 5.9427e-08 - div_term: 1.0771e-04\n",
      "Epoch 149/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 16.2493 - d_loss: 5.6105e-08 - div_term: 1.0797e-04\n",
      "Epoch 150/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 16.3106 - d_loss: 5.3004e-08 - div_term: 1.1097e-04\n",
      "Epoch 151/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 16.3718 - d_loss: 5.0087e-08 - div_term: 1.1390e-04\n",
      "Epoch 152/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 16.4331 - d_loss: 4.7331e-08 - div_term: 1.1244e-04\n",
      "Epoch 153/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 16.4943 - d_loss: 4.4735e-08 - div_term: 1.1195e-04\n",
      "Epoch 154/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 16.5557 - d_loss: 4.2283e-08 - div_term: 1.0779e-04\n",
      "Epoch 155/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 16.6172 - d_loss: 3.9962e-08 - div_term: 9.9729e-05\n",
      "Epoch 156/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: 16.6787 - d_loss: 3.7771e-08 - div_term: 1.0656e-04\n",
      "Epoch 157/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 16.7403 - d_loss: 3.5701e-08 - div_term: 9.8214e-05\n",
      "Epoch 158/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 16.8020 - d_loss: 3.3746e-08 - div_term: 1.0076e-04\n",
      "Epoch 159/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 16.8639 - d_loss: 3.1892e-08 - div_term: 1.0178e-04\n",
      "Epoch 160/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 16.9260 - d_loss: 3.0137e-08 - div_term: 1.0166e-04\n",
      "Epoch 161/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 16.9882 - d_loss: 2.8476e-08 - div_term: 9.7317e-05\n",
      "Epoch 162/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 17.0505 - d_loss: 2.6904e-08 - div_term: 9.9059e-05\n",
      "Epoch 163/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: 17.1130 - d_loss: 2.5414e-08 - div_term: 9.4829e-05\n",
      "Epoch 164/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 17.1757 - d_loss: 2.4002e-08 - div_term: 9.3549e-05\n",
      "Epoch 165/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 17.2384 - d_loss: 2.2669e-08 - div_term: 9.5532e-05\n",
      "Epoch 166/420\n",
      "117/136 [========================>.....] - ETA: 1s - g_loss: 17.2990 - d_loss: 2.1432e-08 - div_term: 9.6236e-05"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    cond_gan = ConditionalGAN(\n",
    "        num_features=NUM_FEATURES,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        discriminator=ConditionalGAN.conditional_discriminator(\n",
    "            hidden_units=SEQ_LENGTH, \n",
    "            seq_length=SEQ_LENGTH, \n",
    "            num_features=NUM_FEATURES,\n",
    "            filters=[32, 64, 32],\n",
    "            activation_function= ACTIVATION,\n",
    "            architecture=D_ARCHITECTURE, \n",
    "            #head_size=wandb.config.head_size#wandb.config.d_architecture\n",
    "            #filters=[wandb.config.filter1, wandb.config.filter2, wandb.config.filter3],\n",
    "            #kernel_sizes=[wandb.config.kernel_size1, wandb.config.kernel_size2, wandb.config.kernel_size3]\n",
    "            ),\n",
    "        generator=ConditionalGAN.conditional_generator(\n",
    "            hidden_units=SEQ_LENGTH, \n",
    "            seq_length=SEQ_LENGTH, \n",
    "            latent_dim=LATENT_DIM,\n",
    "            num_features=NUM_FEATURES,\n",
    "            activation_function=ACTIVATION\n",
    "        )\n",
    "    )\n",
    "    if DP_TRAINING:\n",
    "\n",
    "        mult = 10 # TODO\n",
    "        config[\"noise_multiplier\"] = noise_multipliers[mult]\n",
    "\n",
    "        d_optimizer = dp_optimizer_vectorized.VectorizedDPAdamOptimizer( #vectorized adam am schnellsten\n",
    "            l2_norm_clip=L2_NORM_CLIP,\n",
    "            noise_multiplier=noise_multipliers[mult],\n",
    "            num_microbatches=NUM_MICROBATCHES,\n",
    "            learning_rate=DP_LEARNING_RATE\n",
    "        )\n",
    "    else:\n",
    "        d_optimizer = Adam(learning_rate=LEARNING_RATE, beta_1=0.5) # get_optimizer(0.0002, wandb.config.optimizer)#\n",
    "\n",
    "    g_optimizer = Adam(learning_rate=LEARNING_RATE, beta_1=0.5) # get_optimizer(0.0002, wandb.config.optimizer)#\n",
    "\n",
    "    cond_gan.compile(\n",
    "        d_optimizer= d_optimizer, # Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "        g_optimizer= g_optimizer, # Adam(learning_rate=0.0002, beta_1=0.5), #optimizer\n",
    "        loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    )\n",
    "\n",
    "    print(f\"{cond_gan.d_optimizer} is used\")\n",
    "\n",
    "    history = cond_gan.fit(\n",
    "        dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[\n",
    "            # GANMonitor(\n",
    "            #     trainmos,\n",
    "            #     trainnomos,\n",
    "            #     testmos,\n",
    "            #     testnomos,\n",
    "            #     randomTrainMos,\n",
    "            #     randomTrainNoMos,\n",
    "            #     randomTestMos,\n",
    "            #     randomTestNoMos,\n",
    "            #     num_seq=50,\n",
    "            #     save_path=None,\n",
    "            #     batch_size=BATCH_SIZE,\n",
    "            #     seq_length=SEQ_LENGTH,\n",
    "            #     num_features=NUM_FEATURES,\n",
    "            #     dp=DP_TRAINING,\n",
    "            # )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if DP_TRAINING:\n",
    "        base_path = f\"models/new_cgan/{mult}/\"\n",
    "        cond_gan.generator.save(f\"{base_path}generator\")\n",
    "        cond_gan.discriminator.save(f\"{base_path}discriminator\")\n",
    "    elif LOSO_TRAINING_WITHOUT_SUBJECT:\n",
    "        base_path = f\"models/no_dp/loso/sub{LOSO_TRAINING_WITHOUT_SUBJECT}/{wandb.run.name}/\"\n",
    "        cond_gan.generator.save(f\"{base_path}cgan_generator\")\n",
    "        cond_gan.discriminator.save(f\"{base_path}cgan_discriminator\")\n",
    "    else:\n",
    "        base_path = f\"models/no_dp/{wandb.run.name}/\"\n",
    "        cond_gan.generator.save(f\"{base_path}cgan_generator\")\n",
    "        cond_gan.discriminator.save(f\"{base_path}cgan_discriminator\")\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"say 'C GAN IST FERTIG'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
