{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow_privacy.privacy.optimizers import dp_optimizer_vectorized\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.analysis.compute_noise_from_budget_lib import compute_noise\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "from synthesizers.cgan.model import (\n",
    "    ConditionalGAN, \n",
    "    GANMonitor\n",
    ")\n",
    "from synthesizers.preprocessing.wesad import (\n",
    "    WESADDataset, \n",
    "    LabelType\n",
    ")\n",
    "from synthesizers.utils.training import data_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 1\n",
    "USE_SLIDING_WINDOWS = True\n",
    "\n",
    "# Training Hyperparameters\n",
    "DP_TRAINING = True\n",
    "NUM_FEATURES = 6\n",
    "SEQ_LENGTH = 60\n",
    "LATENT_DIM = SEQ_LENGTH\n",
    "BATCH_SIZE = 8\n",
    "HIDDEN_UNITS = 64\n",
    "EPOCHS = 420\n",
    "ACTIVATION = \"relu\"\n",
    "RANDOM_SEED = 42\n",
    "LEARNING_RATE = 0.0002\n",
    "LOSS_FN = \"binary_cross_entropy\"\n",
    "D_ARCHITECTURE = \"lstm\"\n",
    "LOSO_TRAINING_WITHOUT_SUBJECT = False # \"14\"\n",
    "\n",
    "# DP Training Hyperparameter\n",
    "L2_NORM_CLIP = 1.0\n",
    "NUM_MICROBATCHES = BATCH_SIZE\n",
    "DP_LEARNING_RATE = 1e-3\n",
    "DELTA = 1e-4\n",
    "\n",
    "\n",
    "# Define run config\n",
    "config = {\n",
    "    \"activation_function\": ACTIVATION,\n",
    "    \"hidden_units\": HIDDEN_UNITS,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"num_features\": NUM_FEATURES,\n",
    "    \"seq_length\": SEQ_LENGTH,\n",
    "    \"dp_training\": DP_TRAINING,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"loss_function\": LOSS_FN,\n",
    "    \"d_architecture\": D_ARCHITECTURE,\n",
    "    \"use_sliding_windows\": USE_SLIDING_WINDOWS\n",
    "}\n",
    "\n",
    "if LOSO_TRAINING_WITHOUT_SUBJECT:\n",
    "    config[\"WESAD_WITHOUT_SUBJ\"] = LOSO_TRAINING_WITHOUT_SUBJECT\n",
    "\n",
    "if DP_TRAINING:\n",
    "    config[\"l2_norm_clip\"] = L2_NORM_CLIP\n",
    "    config[\"num_microbatches\"] = NUM_MICROBATCHES\n",
    "    config[\"dp_learning_rate\"] = DP_LEARNING_RATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and split it into stress and non-stress for later testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 60, 6)\n",
      "(64, 60, 6)\n",
      "(610, 60, 6)\n",
      "(153, 60, 6)\n"
     ]
    }
   ],
   "source": [
    "windows = np.load('data/wesad/wesad_windows.npy')\n",
    "labels = np.load('data/wesad/wesad_labels.npy')\n",
    "\n",
    "if USE_SLIDING_WINDOWS:\n",
    "    mos = windows[labels == 1]\n",
    "    non_mos = windows[labels == 0]\n",
    "else:\n",
    "    mos = windows[labels == 1]\n",
    "    non_mos = windows[labels == 0]\n",
    "\n",
    "windows = np.delete(windows, 6, axis=2)\n",
    "mos = np.delete(mos, 6, axis=2)\n",
    "non_mos = np.delete(non_mos, 6, axis=2)\n",
    "\n",
    "num_split = 0.8\n",
    "trainmos, testmos = data_split(mos, num_split)\n",
    "trainnomos, testnomos = data_split(non_mos, num_split)\n",
    "\n",
    "print(trainmos.shape)\n",
    "print(testmos.shape)\n",
    "print(trainnomos.shape)\n",
    "print(testnomos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the noise for privacy guarantees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 1.48% and noise_multiplier = 103.99275642585638 iterated over 56805 steps satisfies differential privacy with eps = 0.1 and delta = 0.0001.\n",
      "DP-SGD with sampling rate = 1.48% and noise_multiplier = 12.393045830417874 iterated over 56805 steps satisfies differential privacy with eps = 1 and delta = 0.0001.\n",
      "DP-SGD with sampling rate = 1.48% and noise_multiplier = 1.8509817962014676 iterated over 56805 steps satisfies differential privacy with eps = 10 and delta = 0.0001.\n",
      "{0.1: 103.99275642585638, 1: 12.393045830417874, 10: 1.8509817962014676}\n",
      "DP-SGD with sampling rate = 1.48% and noise_multiplier = 103.99275642585638 iterated over 56805 steps satisfies differential privacy with eps = 0.1 and delta = 0.0001.\n",
      "The optimal RDP order is 128.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09999999999999426, 128.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get needed noise for target epsilon\n",
    "min_noise = 1e-5\n",
    "target_epsilons = [0.1, 1, 10]\n",
    "noise_multipliers = {target_epsilon : compute_noise(\n",
    "    windows.shape[0] // 2,\n",
    "    BATCH_SIZE,\n",
    "    target_epsilon,\n",
    "    EPOCHS * 2,\n",
    "    DELTA,\n",
    "    min_noise\n",
    ") for target_epsilon in target_epsilons}\n",
    "print(noise_multipliers)\n",
    "\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=windows.shape[0] // 2,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              noise_multiplier=noise_multipliers[target_epsilons[0]],\n",
    "                                              epochs=EPOCHS*2,\n",
    "                                              delta=DELTA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweep Config only for sweep case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create TF dataset from windows and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into tf dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((windows, labels))\n",
    "\n",
    "# Shuffle, cache, and batch the dataset\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "randomTrainMos = tf.random.normal(shape=(trainmos.shape[0], LATENT_DIM))\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "randomTrainNoMos = tf.random.normal(shape=(trainnomos.shape[0], LATENT_DIM))\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "randomTestMos = tf.random.normal(shape=(testmos.shape[0], LATENT_DIM))\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "randomTestNoMos = tf.random.normal(shape=(testnomos.shape[0], LATENT_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training in wandb environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_architecture:  lstm\n",
      "<tensorflow_privacy.privacy.optimizers.dp_optimizer_vectorized.make_vectorized_optimizer_class.<locals>.DPOptimizerClass object at 0x17ef85580> is used\n",
      "Epoch 1/420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 19:01:50.131795: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 15s 60ms/step - g_loss: 0.6568 - d_loss: 0.5912 - div_term: 0.5782\n",
      "Epoch 2/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 2.5667 - d_loss: 0.0515 - div_term: 0.1131\n",
      "Epoch 3/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 5.0414 - d_loss: 0.0046 - div_term: 0.0263\n",
      "Epoch 4/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 5.8839 - d_loss: 0.0021 - div_term: 0.0321\n",
      "Epoch 5/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 6.2546 - d_loss: 0.0578 - div_term: 0.1628\n",
      "Epoch 6/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: -0.0814 - d_loss: 0.6563 - div_term: 0.8287\n",
      "Epoch 7/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 0.2609 - d_loss: 0.6369 - div_term: 0.6330\n",
      "Epoch 8/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: -0.3528 - d_loss: 0.7045 - div_term: 0.9023\n",
      "Epoch 9/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 0.2968 - d_loss: 0.6056 - div_term: 0.4683\n",
      "Epoch 10/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 1.0755 - d_loss: 0.5138 - div_term: 1.0057\n",
      "Epoch 11/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: -0.0098 - d_loss: 0.6928 - div_term: 0.6328\n",
      "Epoch 12/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 0.0696 - d_loss: 0.6681 - div_term: 0.8876\n",
      "Epoch 13/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: -0.3621 - d_loss: 0.6863 - div_term: 0.9881\n",
      "Epoch 14/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: -0.1466 - d_loss: 0.6885 - div_term: 0.8209\n",
      "Epoch 15/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 0.0020 - d_loss: 0.6829 - div_term: 0.7096\n",
      "Epoch 16/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 0.0813 - d_loss: 0.6486 - div_term: 0.8561\n",
      "Epoch 17/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 0.5188 - d_loss: 0.5411 - div_term: 0.6607\n",
      "Epoch 18/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 0.8033 - d_loss: 0.3346 - div_term: 0.7250\n",
      "Epoch 19/420\n",
      "136/136 [==============================] - 8s 62ms/step - g_loss: 0.6178 - d_loss: 0.5727 - div_term: 0.4826\n",
      "Epoch 20/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 0.4269 - d_loss: 0.6913 - div_term: 0.3181\n",
      "Epoch 21/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 0.3799 - d_loss: 0.6988 - div_term: 0.3184\n",
      "Epoch 22/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 0.4485 - d_loss: 0.6886 - div_term: 0.3076\n",
      "Epoch 23/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.2021 - d_loss: 0.6820 - div_term: 0.4385\n",
      "Epoch 24/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 0.5694 - d_loss: 0.6762 - div_term: 0.3216\n",
      "Epoch 25/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 0.2829 - d_loss: 0.7045 - div_term: 0.4597\n",
      "Epoch 26/420\n",
      "136/136 [==============================] - 9s 63ms/step - g_loss: 0.1417 - d_loss: 0.6948 - div_term: 0.4852\n",
      "Epoch 27/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 0.3434 - d_loss: 0.6914 - div_term: 0.3491\n",
      "Epoch 28/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 0.4219 - d_loss: 0.6577 - div_term: 0.3021\n",
      "Epoch 29/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 0.6708 - d_loss: 0.6142 - div_term: 0.1980\n",
      "Epoch 30/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: 0.4714 - d_loss: 0.6960 - div_term: 0.2400\n",
      "Epoch 31/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.6806 - d_loss: 0.6448 - div_term: 0.1978\n",
      "Epoch 32/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 0.6698 - d_loss: 0.5859 - div_term: 0.5852\n",
      "Epoch 33/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.0638 - d_loss: 0.6272 - div_term: 0.8358\n",
      "Epoch 34/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 0.1120 - d_loss: 0.5807 - div_term: 0.9121\n",
      "Epoch 35/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: -0.0762 - d_loss: 0.5574 - div_term: 1.1681\n",
      "Epoch 36/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: -0.0610 - d_loss: 0.5656 - div_term: 1.0023\n",
      "Epoch 37/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: -0.2098 - d_loss: 0.6136 - div_term: 1.0054\n",
      "Epoch 38/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: 0.0824 - d_loss: 0.5805 - div_term: 0.7645\n",
      "Epoch 39/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 0.3201 - d_loss: 0.6117 - div_term: 0.6305\n",
      "Epoch 40/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.2514 - d_loss: 0.5472 - div_term: 0.7092\n",
      "Epoch 41/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 0.2734 - d_loss: 0.5940 - div_term: 0.5890\n",
      "Epoch 42/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: -0.1759 - d_loss: 0.6331 - div_term: 1.2211\n",
      "Epoch 43/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: -0.2019 - d_loss: 0.6176 - div_term: 1.1214\n",
      "Epoch 44/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: -0.5621 - d_loss: 0.6107 - div_term: 1.1305\n",
      "Epoch 45/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: -0.2989 - d_loss: 0.7272 - div_term: 1.0562\n",
      "Epoch 46/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: -0.6197 - d_loss: 0.6679 - div_term: 1.3031\n",
      "Epoch 47/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: -0.2094 - d_loss: 0.6640 - div_term: 0.9064\n",
      "Epoch 48/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: -0.3002 - d_loss: 0.6729 - div_term: 1.2806\n",
      "Epoch 49/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: -0.7864 - d_loss: 0.6925 - div_term: 1.4528\n",
      "Epoch 50/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.7969 - d_loss: 0.6896 - div_term: 1.4571\n",
      "Epoch 51/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: -0.4090 - d_loss: 0.6869 - div_term: 1.0126\n",
      "Epoch 52/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.0182 - d_loss: 0.6860 - div_term: 0.6308\n",
      "Epoch 53/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: 0.1041 - d_loss: 0.6832 - div_term: 0.5915\n",
      "Epoch 54/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.0653 - d_loss: 0.6854 - div_term: 0.6604\n",
      "Epoch 55/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: 0.1553 - d_loss: 0.6881 - div_term: 0.5712\n",
      "Epoch 56/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.0723 - d_loss: 0.6666 - div_term: 0.6479\n",
      "Epoch 57/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.4117 - d_loss: 0.6848 - div_term: 0.3760\n",
      "Epoch 58/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.3716 - d_loss: 0.6348 - div_term: 0.5676\n",
      "Epoch 59/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 0.7682 - d_loss: 0.4868 - div_term: 0.3144\n",
      "Epoch 60/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 0.7059 - d_loss: 0.9236 - div_term: 0.5304\n",
      "Epoch 61/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 0.2476 - d_loss: 0.5486 - div_term: 0.6971\n",
      "Epoch 62/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.0036 - d_loss: 0.6806 - div_term: 0.7780\n",
      "Epoch 63/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.1456 - d_loss: 0.6812 - div_term: 0.5888\n",
      "Epoch 64/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.0747 - d_loss: 0.6825 - div_term: 0.8288\n",
      "Epoch 65/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 0.1174 - d_loss: 0.6430 - div_term: 0.6246\n",
      "Epoch 66/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.1407 - d_loss: 0.6938 - div_term: 0.5752\n",
      "Epoch 67/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 0.2157 - d_loss: 0.6630 - div_term: 0.5456\n",
      "Epoch 68/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 0.3636 - d_loss: 0.6535 - div_term: 0.5672\n",
      "Epoch 69/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.4696 - d_loss: 0.5953 - div_term: 0.4088\n",
      "Epoch 70/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.6245 - d_loss: 0.5379 - div_term: 0.9400\n",
      "Epoch 71/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: -0.1104 - d_loss: 0.5843 - div_term: 0.9348\n",
      "Epoch 72/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 0.2402 - d_loss: 0.5938 - div_term: 0.6474\n",
      "Epoch 73/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.4414 - d_loss: 0.5616 - div_term: 0.5897\n",
      "Epoch 74/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.5271 - d_loss: 0.5240 - div_term: 0.5776\n",
      "Epoch 75/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 0.4651 - d_loss: 0.5855 - div_term: 0.4843\n",
      "Epoch 76/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: 0.5391 - d_loss: 0.6132 - div_term: 0.5831\n",
      "Epoch 77/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: 0.7915 - d_loss: 0.5356 - div_term: 0.6964\n",
      "Epoch 78/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 0.7909 - d_loss: 0.4429 - div_term: 0.5503\n",
      "Epoch 79/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.0241 - d_loss: 0.5560 - div_term: 1.5552\n",
      "Epoch 80/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 0.0585 - d_loss: 0.5795 - div_term: 1.0031\n",
      "Epoch 81/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: -0.0830 - d_loss: 0.6629 - div_term: 0.8943\n",
      "Epoch 82/420\n",
      "136/136 [==============================] - 9s 64ms/step - g_loss: -0.0824 - d_loss: 0.5886 - div_term: 1.2579\n",
      "Epoch 83/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.0942 - d_loss: 0.5081 - div_term: 1.2709\n",
      "Epoch 84/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.2140 - d_loss: 0.5962 - div_term: 1.2195\n",
      "Epoch 85/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.2274 - d_loss: 0.5295 - div_term: 1.4796\n",
      "Epoch 86/420\n",
      "136/136 [==============================] - 9s 65ms/step - g_loss: -0.0424 - d_loss: 0.5686 - div_term: 1.2279\n",
      "Epoch 87/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.1833 - d_loss: 0.5748 - div_term: 1.3518\n",
      "Epoch 88/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.4579 - d_loss: 0.5878 - div_term: 1.3494\n",
      "Epoch 89/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.1998 - d_loss: 0.5628 - div_term: 1.1999\n",
      "Epoch 90/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 0.1434 - d_loss: 0.5491 - div_term: 1.1196\n",
      "Epoch 91/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 0.0930 - d_loss: 0.5164 - div_term: 1.0919\n",
      "Epoch 92/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.1032 - d_loss: 0.6104 - div_term: 1.2741\n",
      "Epoch 93/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.3834 - d_loss: 0.5347 - div_term: 1.3228\n",
      "Epoch 94/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.3608 - d_loss: 0.5501 - div_term: 1.4537\n",
      "Epoch 95/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.4195 - d_loss: 0.5972 - div_term: 1.5018\n",
      "Epoch 96/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.6165 - d_loss: 0.5261 - div_term: 1.6085\n",
      "Epoch 97/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.3832 - d_loss: 0.5205 - div_term: 1.6548\n",
      "Epoch 98/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.8662 - d_loss: 0.5397 - div_term: 2.3462\n",
      "Epoch 99/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -1.5599 - d_loss: 0.5020 - div_term: 2.4872\n",
      "Epoch 100/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.5401 - d_loss: 0.4439 - div_term: 1.7987\n",
      "Epoch 101/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.1381 - d_loss: 0.4803 - div_term: 1.5730\n",
      "Epoch 102/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: 0.0640 - d_loss: 0.4923 - div_term: 1.4503\n",
      "Epoch 103/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.4326 - d_loss: 0.5460 - div_term: 1.5950\n",
      "Epoch 104/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.4878 - d_loss: 0.4867 - div_term: 1.6909\n",
      "Epoch 105/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.8493 - d_loss: 0.5267 - div_term: 2.0552\n",
      "Epoch 106/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.2310 - d_loss: 0.4819 - div_term: 1.7000\n",
      "Epoch 107/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.2002 - d_loss: 0.4786 - div_term: 1.5546\n",
      "Epoch 108/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.1230 - d_loss: 0.5365 - div_term: 1.4313\n",
      "Epoch 109/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.2559 - d_loss: 0.5724 - div_term: 1.3906\n",
      "Epoch 110/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.3662 - d_loss: 0.5665 - div_term: 1.4061\n",
      "Epoch 111/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.3674 - d_loss: 0.5531 - div_term: 1.4363\n",
      "Epoch 112/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.4259 - d_loss: 0.5485 - div_term: 1.5279\n",
      "Epoch 113/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.1911 - d_loss: 0.5279 - div_term: 1.3209\n",
      "Epoch 114/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.1588 - d_loss: 0.4944 - div_term: 1.5163\n",
      "Epoch 115/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.4083 - d_loss: 0.5062 - div_term: 1.7663\n",
      "Epoch 116/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.1832 - d_loss: 0.5200 - div_term: 1.2622\n",
      "Epoch 117/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.5182 - d_loss: 0.5203 - div_term: 1.9332\n",
      "Epoch 118/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.6166 - d_loss: 0.5075 - div_term: 1.8652\n",
      "Epoch 119/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.5877 - d_loss: 0.5053 - div_term: 1.8724\n",
      "Epoch 120/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -1.0560 - d_loss: 0.5976 - div_term: 2.3288\n",
      "Epoch 121/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -1.7581 - d_loss: 0.5964 - div_term: 2.6490\n",
      "Epoch 122/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.9433 - d_loss: 0.5637 - div_term: 2.1475\n",
      "Epoch 123/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.8695 - d_loss: 0.6294 - div_term: 1.7444\n",
      "Epoch 124/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.6358 - d_loss: 0.5552 - div_term: 1.8019\n",
      "Epoch 125/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.5567 - d_loss: 0.5439 - div_term: 1.6254\n",
      "Epoch 126/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.9178 - d_loss: 0.5851 - div_term: 1.9562\n",
      "Epoch 127/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.8959 - d_loss: 0.5789 - div_term: 1.8739\n",
      "Epoch 128/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.6449 - d_loss: 0.5344 - div_term: 1.8323\n",
      "Epoch 129/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.5610 - d_loss: 0.5384 - div_term: 1.7492\n",
      "Epoch 130/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.5262 - d_loss: 0.5290 - div_term: 1.6595\n",
      "Epoch 131/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: -0.4351 - d_loss: 0.5551 - div_term: 1.6766\n",
      "Epoch 132/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.8447 - d_loss: 0.5663 - div_term: 1.9358\n",
      "Epoch 133/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.4526 - d_loss: 0.5632 - div_term: 1.5728\n",
      "Epoch 134/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.6459 - d_loss: 0.5482 - div_term: 1.9390\n",
      "Epoch 135/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.9442 - d_loss: 0.5818 - div_term: 1.8664\n",
      "Epoch 136/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.7431 - d_loss: 0.5841 - div_term: 1.6547\n",
      "Epoch 137/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.3695 - d_loss: 0.5500 - div_term: 1.5311\n",
      "Epoch 138/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.7171 - d_loss: 0.5640 - div_term: 1.8000\n",
      "Epoch 139/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.9846 - d_loss: 0.5548 - div_term: 1.9584\n",
      "Epoch 140/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.4212 - d_loss: 0.5041 - div_term: 1.6675\n",
      "Epoch 141/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.6322 - d_loss: 0.5126 - div_term: 1.8411\n",
      "Epoch 142/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.7627 - d_loss: 0.5425 - div_term: 1.9607\n",
      "Epoch 143/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.7322 - d_loss: 0.5604 - div_term: 1.7507\n",
      "Epoch 144/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.5610 - d_loss: 0.5408 - div_term: 1.8597\n",
      "Epoch 145/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.9819 - d_loss: 0.5705 - div_term: 2.0369\n",
      "Epoch 146/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.6507 - d_loss: 0.5954 - div_term: 1.7350\n",
      "Epoch 147/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.8644 - d_loss: 0.5927 - div_term: 1.8259\n",
      "Epoch 148/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.5817 - d_loss: 0.5426 - div_term: 1.8759\n",
      "Epoch 149/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.7933 - d_loss: 0.5322 - div_term: 1.9435\n",
      "Epoch 150/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.6381 - d_loss: 0.5813 - div_term: 1.4153\n",
      "Epoch 151/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.3147 - d_loss: 0.5339 - div_term: 1.6011\n",
      "Epoch 152/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.4503 - d_loss: 0.5222 - div_term: 1.6309\n",
      "Epoch 153/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.6653 - d_loss: 0.5781 - div_term: 1.7947\n",
      "Epoch 154/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: -0.5435 - d_loss: 0.5999 - div_term: 1.5618\n",
      "Epoch 155/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: -0.8313 - d_loss: 0.6247 - div_term: 1.6903\n",
      "Epoch 156/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.8521 - d_loss: 0.6458 - div_term: 1.6530\n",
      "Epoch 157/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: -1.0148 - d_loss: 0.6718 - div_term: 1.8734\n",
      "Epoch 158/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -1.0750 - d_loss: 0.6330 - div_term: 1.8966\n",
      "Epoch 159/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.9968 - d_loss: 0.6558 - div_term: 1.8466\n",
      "Epoch 160/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.9503 - d_loss: 0.6045 - div_term: 1.8292\n",
      "Epoch 161/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -1.1508 - d_loss: 0.6254 - div_term: 2.1324\n",
      "Epoch 162/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -1.2415 - d_loss: 0.6209 - div_term: 1.9583\n",
      "Epoch 163/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -1.1807 - d_loss: 0.6305 - div_term: 2.0331\n",
      "Epoch 164/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -1.2518 - d_loss: 0.6104 - div_term: 2.0715\n",
      "Epoch 165/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.9378 - d_loss: 0.6302 - div_term: 1.9099\n",
      "Epoch 166/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -1.2124 - d_loss: 0.6638 - div_term: 2.0982\n",
      "Epoch 167/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: -1.0316 - d_loss: 0.6197 - div_term: 1.8896\n",
      "Epoch 168/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -1.0399 - d_loss: 0.5818 - div_term: 1.9531\n",
      "Epoch 169/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.6797 - d_loss: 0.6635 - div_term: 1.5523\n",
      "Epoch 170/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.7364 - d_loss: 0.6130 - div_term: 1.7067\n",
      "Epoch 171/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.7433 - d_loss: 0.6316 - div_term: 1.5855\n",
      "Epoch 172/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.5858 - d_loss: 0.6219 - div_term: 1.6829\n",
      "Epoch 173/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.7277 - d_loss: 0.6195 - div_term: 1.8074\n",
      "Epoch 174/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: -0.6317 - d_loss: 0.6199 - div_term: 1.6353\n",
      "Epoch 175/420\n",
      "136/136 [==============================] - 9s 66ms/step - g_loss: -0.6164 - d_loss: 0.5879 - div_term: 1.5215\n",
      "Epoch 176/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.8618 - d_loss: 0.6292 - div_term: 1.6752\n",
      "Epoch 177/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.5758 - d_loss: 0.6155 - div_term: 1.5696\n",
      "Epoch 178/420\n",
      "136/136 [==============================] - 11s 80ms/step - g_loss: -0.6211 - d_loss: 0.5874 - div_term: 1.6845\n",
      "Epoch 179/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.5392 - d_loss: 0.5790 - div_term: 1.5050\n",
      "Epoch 180/420\n",
      "136/136 [==============================] - 12s 88ms/step - g_loss: -0.5766 - d_loss: 0.5710 - div_term: 1.7536\n",
      "Epoch 181/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.5666 - d_loss: 0.5497 - div_term: 1.6655\n",
      "Epoch 182/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.4428 - d_loss: 0.5454 - div_term: 1.6924\n",
      "Epoch 183/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.5836 - d_loss: 0.5435 - div_term: 1.8985\n",
      "Epoch 184/420\n",
      "136/136 [==============================] - 12s 92ms/step - g_loss: -0.6725 - d_loss: 0.5142 - div_term: 1.8526\n",
      "Epoch 185/420\n",
      "136/136 [==============================] - 17s 127ms/step - g_loss: -0.6228 - d_loss: 0.4914 - div_term: 1.9418\n",
      "Epoch 186/420\n",
      "136/136 [==============================] - 13s 92ms/step - g_loss: -0.6259 - d_loss: 0.4866 - div_term: 1.9284\n",
      "Epoch 187/420\n",
      "136/136 [==============================] - 14s 100ms/step - g_loss: -0.4938 - d_loss: 0.4879 - div_term: 1.9179\n",
      "Epoch 188/420\n",
      "136/136 [==============================] - 12s 87ms/step - g_loss: -0.4772 - d_loss: 0.5081 - div_term: 1.9004\n",
      "Epoch 189/420\n",
      "136/136 [==============================] - 11s 80ms/step - g_loss: -0.3923 - d_loss: 0.4670 - div_term: 1.8804\n",
      "Epoch 190/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.4944 - d_loss: 0.5052 - div_term: 2.0866\n",
      "Epoch 191/420\n",
      "136/136 [==============================] - 11s 78ms/step - g_loss: -0.4470 - d_loss: 0.4718 - div_term: 1.8856\n",
      "Epoch 192/420\n",
      "136/136 [==============================] - 11s 77ms/step - g_loss: -0.4051 - d_loss: 0.4413 - div_term: 1.8737\n",
      "Epoch 193/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: -0.4421 - d_loss: 0.5102 - div_term: 1.8668\n",
      "Epoch 194/420\n",
      "136/136 [==============================] - 11s 80ms/step - g_loss: -0.4806 - d_loss: 0.4949 - div_term: 1.7845\n",
      "Epoch 195/420\n",
      "136/136 [==============================] - 13s 98ms/step - g_loss: -0.4737 - d_loss: 0.5008 - div_term: 1.8881\n",
      "Epoch 196/420\n",
      "136/136 [==============================] - 11s 78ms/step - g_loss: -0.2914 - d_loss: 0.4976 - div_term: 1.7674\n",
      "Epoch 197/420\n",
      "136/136 [==============================] - 11s 83ms/step - g_loss: -0.3443 - d_loss: 0.4830 - div_term: 1.7345\n",
      "Epoch 198/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.4554 - d_loss: 0.6332 - div_term: 1.6259\n",
      "Epoch 199/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.3364 - d_loss: 0.5147 - div_term: 1.7031\n",
      "Epoch 200/420\n",
      "136/136 [==============================] - 10s 77ms/step - g_loss: -0.1896 - d_loss: 0.4738 - div_term: 1.7910\n",
      "Epoch 201/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.2381 - d_loss: 0.4446 - div_term: 1.7937\n",
      "Epoch 202/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.2336 - d_loss: 0.4741 - div_term: 1.9102\n",
      "Epoch 203/420\n",
      "136/136 [==============================] - 11s 79ms/step - g_loss: -0.5610 - d_loss: 0.5463 - div_term: 1.8244\n",
      "Epoch 204/420\n",
      "136/136 [==============================] - 10s 76ms/step - g_loss: -0.6192 - d_loss: 0.5402 - div_term: 1.7703\n",
      "Epoch 205/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.3975 - d_loss: 0.5200 - div_term: 1.7155\n",
      "Epoch 206/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.3170 - d_loss: 0.4962 - div_term: 1.7196\n",
      "Epoch 207/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.3952 - d_loss: 0.5001 - div_term: 1.8981\n",
      "Epoch 208/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.3867 - d_loss: 0.4947 - div_term: 1.9046\n",
      "Epoch 209/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: -0.1816 - d_loss: 0.4484 - div_term: 1.6738\n",
      "Epoch 210/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: 0.0305 - d_loss: 0.4566 - div_term: 1.6777\n",
      "Epoch 211/420\n",
      "136/136 [==============================] - 10s 77ms/step - g_loss: -0.2487 - d_loss: 0.4764 - div_term: 1.6896\n",
      "Epoch 212/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.0140 - d_loss: 0.4825 - div_term: 1.6891\n",
      "Epoch 213/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.2281 - d_loss: 0.4864 - div_term: 1.7710\n",
      "Epoch 214/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.2163 - d_loss: 0.4600 - div_term: 1.8907\n",
      "Epoch 215/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.2239 - d_loss: 0.4563 - div_term: 1.7191\n",
      "Epoch 216/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 0.0275 - d_loss: 0.4295 - div_term: 1.6610\n",
      "Epoch 217/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.2475 - d_loss: 0.4673 - div_term: 1.8933\n",
      "Epoch 218/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4881 - d_loss: 0.4542 - div_term: 1.8968\n",
      "Epoch 219/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.3464 - d_loss: 0.4523 - div_term: 1.9046\n",
      "Epoch 220/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.3444 - d_loss: 0.4573 - div_term: 1.8990\n",
      "Epoch 221/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.2912 - d_loss: 0.3982 - div_term: 1.8324\n",
      "Epoch 222/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.2852 - d_loss: 0.4570 - div_term: 1.8639\n",
      "Epoch 223/420\n",
      "136/136 [==============================] - 10s 76ms/step - g_loss: -0.2601 - d_loss: 0.4396 - div_term: 1.7814\n",
      "Epoch 224/420\n",
      "136/136 [==============================] - 11s 78ms/step - g_loss: -0.1626 - d_loss: 0.4555 - div_term: 1.7200\n",
      "Epoch 225/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.1239 - d_loss: 0.4015 - div_term: 1.7723\n",
      "Epoch 226/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1288 - d_loss: 0.4173 - div_term: 1.8455\n",
      "Epoch 227/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4731 - d_loss: 0.4249 - div_term: 1.9888\n",
      "Epoch 228/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1957 - d_loss: 0.4002 - div_term: 1.9222\n",
      "Epoch 229/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.2472 - d_loss: 0.3998 - div_term: 1.9051\n",
      "Epoch 230/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.2122 - d_loss: 0.3751 - div_term: 1.8766\n",
      "Epoch 231/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.0823 - d_loss: 0.4127 - div_term: 1.7594\n",
      "Epoch 232/420\n",
      "136/136 [==============================] - 14s 100ms/step - g_loss: -0.3306 - d_loss: 0.4425 - div_term: 1.9819\n",
      "Epoch 233/420\n",
      "136/136 [==============================] - 16s 119ms/step - g_loss: -0.3987 - d_loss: 0.4462 - div_term: 2.1349\n",
      "Epoch 234/420\n",
      "136/136 [==============================] - 15s 112ms/step - g_loss: -0.5108 - d_loss: 0.4147 - div_term: 2.0885\n",
      "Epoch 235/420\n",
      "136/136 [==============================] - 12s 86ms/step - g_loss: -0.4301 - d_loss: 0.4075 - div_term: 2.0286\n",
      "Epoch 236/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: -0.1994 - d_loss: 0.4005 - div_term: 1.9517\n",
      "Epoch 237/420\n",
      "136/136 [==============================] - 14s 103ms/step - g_loss: 0.0069 - d_loss: 0.4545 - div_term: 1.7675\n",
      "Epoch 238/420\n",
      "136/136 [==============================] - 13s 93ms/step - g_loss: -0.0843 - d_loss: 0.4017 - div_term: 1.9046\n",
      "Epoch 239/420\n",
      "136/136 [==============================] - 11s 81ms/step - g_loss: -0.4431 - d_loss: 0.3897 - div_term: 1.9787\n",
      "Epoch 240/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.0842 - d_loss: 0.4087 - div_term: 1.7296\n",
      "Epoch 241/420\n",
      "136/136 [==============================] - 11s 78ms/step - g_loss: -0.1044 - d_loss: 0.4147 - div_term: 1.6936\n",
      "Epoch 242/420\n",
      "136/136 [==============================] - 14s 102ms/step - g_loss: -0.0512 - d_loss: 0.4549 - div_term: 1.6711\n",
      "Epoch 243/420\n",
      "136/136 [==============================] - 11s 83ms/step - g_loss: -0.4186 - d_loss: 0.4571 - div_term: 1.8324\n",
      "Epoch 244/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.4619 - d_loss: 0.4178 - div_term: 1.8718\n",
      "Epoch 245/420\n",
      "136/136 [==============================] - 13s 94ms/step - g_loss: -0.1282 - d_loss: 0.4240 - div_term: 1.7170\n",
      "Epoch 246/420\n",
      "136/136 [==============================] - 11s 82ms/step - g_loss: -0.1066 - d_loss: 0.4065 - div_term: 1.6565\n",
      "Epoch 247/420\n",
      "136/136 [==============================] - 12s 87ms/step - g_loss: -0.0420 - d_loss: 0.3877 - div_term: 1.7390\n",
      "Epoch 248/420\n",
      "136/136 [==============================] - 10s 76ms/step - g_loss: -0.2906 - d_loss: 0.3967 - div_term: 1.7862\n",
      "Epoch 249/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.2216 - d_loss: 0.4613 - div_term: 1.7537\n",
      "Epoch 250/420\n",
      "136/136 [==============================] - 10s 76ms/step - g_loss: -0.6790 - d_loss: 0.5227 - div_term: 1.7512\n",
      "Epoch 251/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.6560 - d_loss: 0.5207 - div_term: 1.8584\n",
      "Epoch 252/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.4933 - d_loss: 0.5154 - div_term: 1.6033\n",
      "Epoch 253/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1958 - d_loss: 0.4721 - div_term: 1.4039\n",
      "Epoch 254/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.2764 - d_loss: 0.4935 - div_term: 1.6072\n",
      "Epoch 255/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.3847 - d_loss: 0.4560 - div_term: 1.6098\n",
      "Epoch 256/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.0810 - d_loss: 0.4406 - div_term: 1.4913\n",
      "Epoch 257/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1804 - d_loss: 0.4463 - div_term: 1.6006\n",
      "Epoch 258/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4046 - d_loss: 0.4392 - div_term: 1.8542\n",
      "Epoch 259/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.5464 - d_loss: 0.4569 - div_term: 1.8932\n",
      "Epoch 260/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.6011 - d_loss: 0.4772 - div_term: 2.0828\n",
      "Epoch 261/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -1.0064 - d_loss: 0.5621 - div_term: 1.9969\n",
      "Epoch 262/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -1.0314 - d_loss: 0.5527 - div_term: 1.9828\n",
      "Epoch 263/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.6589 - d_loss: 0.5378 - div_term: 1.7688\n",
      "Epoch 264/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.5945 - d_loss: 0.5277 - div_term: 1.7053\n",
      "Epoch 265/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.3720 - d_loss: 0.5095 - div_term: 1.6876\n",
      "Epoch 266/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.5701 - d_loss: 0.5409 - div_term: 1.7923\n",
      "Epoch 267/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.7390 - d_loss: 0.5457 - div_term: 1.8290\n",
      "Epoch 268/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.5365 - d_loss: 0.5189 - div_term: 1.7055\n",
      "Epoch 269/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4327 - d_loss: 0.5341 - div_term: 1.5996\n",
      "Epoch 270/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.5008 - d_loss: 0.5979 - div_term: 1.5616\n",
      "Epoch 271/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4492 - d_loss: 0.5271 - div_term: 1.6256\n",
      "Epoch 272/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.3540 - d_loss: 0.5047 - div_term: 1.5941\n",
      "Epoch 273/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.2968 - d_loss: 0.4939 - div_term: 1.5538\n",
      "Epoch 274/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.4129 - d_loss: 0.4923 - div_term: 1.6152\n",
      "Epoch 275/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.3175 - d_loss: 0.4788 - div_term: 1.6070\n",
      "Epoch 276/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.2364 - d_loss: 0.4564 - div_term: 1.6631\n",
      "Epoch 277/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.3288 - d_loss: 0.4548 - div_term: 1.6292\n",
      "Epoch 278/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1533 - d_loss: 0.4784 - div_term: 1.4958\n",
      "Epoch 279/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: -0.2922 - d_loss: 0.4579 - div_term: 1.6723\n",
      "Epoch 280/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.5163 - d_loss: 0.4918 - div_term: 1.8664\n",
      "Epoch 281/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4712 - d_loss: 0.4626 - div_term: 1.8755\n",
      "Epoch 282/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.4416 - d_loss: 0.4192 - div_term: 1.8913\n",
      "Epoch 283/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.3709 - d_loss: 0.4408 - div_term: 1.8149\n",
      "Epoch 284/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: -0.3401 - d_loss: 0.4547 - div_term: 1.8362\n",
      "Epoch 285/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.5378 - d_loss: 0.4646 - div_term: 1.9692\n",
      "Epoch 286/420\n",
      "136/136 [==============================] - 11s 82ms/step - g_loss: -0.5769 - d_loss: 0.4651 - div_term: 1.9373\n",
      "Epoch 287/420\n",
      "136/136 [==============================] - 10s 77ms/step - g_loss: -0.4704 - d_loss: 0.4467 - div_term: 1.8930\n",
      "Epoch 288/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.3916 - d_loss: 0.4496 - div_term: 1.8077\n",
      "Epoch 289/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4494 - d_loss: 0.4329 - div_term: 1.9527\n",
      "Epoch 290/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.4829 - d_loss: 0.4360 - div_term: 1.9508\n",
      "Epoch 291/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4042 - d_loss: 0.5014 - div_term: 1.8466\n",
      "Epoch 292/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.7235 - d_loss: 0.5230 - div_term: 1.8775\n",
      "Epoch 293/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.3999 - d_loss: 0.4723 - div_term: 1.7335\n",
      "Epoch 294/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.5739 - d_loss: 0.4694 - div_term: 1.8375\n",
      "Epoch 295/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: -0.2254 - d_loss: 0.4577 - div_term: 1.6744\n",
      "Epoch 296/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: -0.3026 - d_loss: 0.4555 - div_term: 1.7826\n",
      "Epoch 297/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.2873 - d_loss: 0.4313 - div_term: 1.7470\n",
      "Epoch 298/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.1249 - d_loss: 0.4449 - div_term: 1.7311\n",
      "Epoch 299/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.2776 - d_loss: 0.4291 - div_term: 1.7394\n",
      "Epoch 300/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.0131 - d_loss: 0.4472 - div_term: 1.6107\n",
      "Epoch 301/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 0.0602 - d_loss: 0.4388 - div_term: 1.6498\n",
      "Epoch 302/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 0.1247 - d_loss: 0.4503 - div_term: 1.5245\n",
      "Epoch 303/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1009 - d_loss: 0.4294 - div_term: 1.6208\n",
      "Epoch 304/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.2045 - d_loss: 0.4525 - div_term: 1.7878\n",
      "Epoch 305/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.1548 - d_loss: 0.4322 - div_term: 1.6084\n",
      "Epoch 306/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.0587 - d_loss: 0.4385 - div_term: 1.6641\n",
      "Epoch 307/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1945 - d_loss: 0.4297 - div_term: 1.7189\n",
      "Epoch 308/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.3195 - d_loss: 0.4708 - div_term: 1.8587\n",
      "Epoch 309/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.5407 - d_loss: 0.4534 - div_term: 1.8416\n",
      "Epoch 310/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.2639 - d_loss: 0.4460 - div_term: 1.7672\n",
      "Epoch 311/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: -0.5975 - d_loss: 0.5555 - div_term: 1.7672\n",
      "Epoch 312/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: -0.4450 - d_loss: 0.5048 - div_term: 1.8385\n",
      "Epoch 313/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4310 - d_loss: 0.4829 - div_term: 1.7634\n",
      "Epoch 314/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.2686 - d_loss: 0.4410 - div_term: 1.7228\n",
      "Epoch 315/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.2150 - d_loss: 0.4483 - div_term: 1.7657\n",
      "Epoch 316/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.2997 - d_loss: 0.4473 - div_term: 1.8244\n",
      "Epoch 317/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1239 - d_loss: 0.4416 - div_term: 1.6606\n",
      "Epoch 318/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.3083 - d_loss: 0.4578 - div_term: 1.8023\n",
      "Epoch 319/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.3671 - d_loss: 0.4382 - div_term: 1.9087\n",
      "Epoch 320/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4972 - d_loss: 0.4306 - div_term: 1.8654\n",
      "Epoch 321/420\n",
      "136/136 [==============================] - 1176s 9s/step - g_loss: 0.0134 - d_loss: 0.4474 - div_term: 1.6576\n",
      "Epoch 322/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.0100 - d_loss: 0.4124 - div_term: 1.6204\n",
      "Epoch 323/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 0.0941 - d_loss: 0.3987 - div_term: 1.5319\n",
      "Epoch 324/420\n",
      "136/136 [==============================] - 3949s 29s/step - g_loss: -0.1650 - d_loss: 0.4166 - div_term: 1.7178\n",
      "Epoch 325/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.0861 - d_loss: 0.4391 - div_term: 1.6776\n",
      "Epoch 326/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.0544 - d_loss: 0.3872 - div_term: 1.6296\n",
      "Epoch 327/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.1496 - d_loss: 0.3939 - div_term: 1.7036\n",
      "Epoch 328/420\n",
      "136/136 [==============================] - 255s 2s/step - g_loss: -0.0675 - d_loss: 0.4129 - div_term: 1.6717\n",
      "Epoch 329/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.0168 - d_loss: 0.3805 - div_term: 1.7074\n",
      "Epoch 330/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.2141 - d_loss: 0.4098 - div_term: 1.8015\n",
      "Epoch 331/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.4932 - d_loss: 0.4651 - div_term: 1.7932\n",
      "Epoch 332/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1141 - d_loss: 0.4249 - div_term: 1.7172\n",
      "Epoch 333/420\n",
      "136/136 [==============================] - 501s 4s/step - g_loss: -0.0951 - d_loss: 0.4003 - div_term: 1.7292\n",
      "Epoch 334/420\n",
      "136/136 [==============================] - 10s 75ms/step - g_loss: -0.1099 - d_loss: 0.4143 - div_term: 1.7127\n",
      "Epoch 335/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.0524 - d_loss: 0.3918 - div_term: 1.8007\n",
      "Epoch 336/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.2837 - d_loss: 0.3991 - div_term: 1.9584\n",
      "Epoch 337/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.2045 - d_loss: 0.3993 - div_term: 1.8366\n",
      "Epoch 338/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.1970 - d_loss: 0.3991 - div_term: 1.8585\n",
      "Epoch 339/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.1516 - d_loss: 0.3927 - div_term: 1.8782\n",
      "Epoch 340/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.0542 - d_loss: 0.3846 - div_term: 1.7167\n",
      "Epoch 341/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.5898 - d_loss: 0.4561 - div_term: 1.9742\n",
      "Epoch 342/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.5907 - d_loss: 0.4234 - div_term: 2.1146\n",
      "Epoch 343/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.2738 - d_loss: 0.3967 - div_term: 1.9809\n",
      "Epoch 344/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.0807 - d_loss: 0.3885 - div_term: 1.8711\n",
      "Epoch 345/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.0436 - d_loss: 0.3930 - div_term: 1.7864\n",
      "Epoch 346/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: 0.0514 - d_loss: 0.3990 - div_term: 1.6981\n",
      "Epoch 347/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.1076 - d_loss: 0.3708 - div_term: 1.6847\n",
      "Epoch 348/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.1594 - d_loss: 0.3889 - div_term: 1.5626\n",
      "Epoch 349/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.2172 - d_loss: 0.3950 - div_term: 1.5949\n",
      "Epoch 350/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.3680 - d_loss: 0.3973 - div_term: 1.5466\n",
      "Epoch 351/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.1001 - d_loss: 0.3745 - div_term: 1.7416\n",
      "Epoch 352/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.0310 - d_loss: 0.3587 - div_term: 1.8376\n",
      "Epoch 353/420\n",
      "136/136 [==============================] - 180s 1s/step - g_loss: -0.0719 - d_loss: 0.4119 - div_term: 1.8109\n",
      "Epoch 354/420\n",
      "136/136 [==============================] - 10s 77ms/step - g_loss: -0.1125 - d_loss: 0.3628 - div_term: 1.7954\n",
      "Epoch 355/420\n",
      "136/136 [==============================] - 11s 78ms/step - g_loss: 0.1024 - d_loss: 0.3589 - div_term: 1.8184\n",
      "Epoch 356/420\n",
      "136/136 [==============================] - 10s 74ms/step - g_loss: -0.0637 - d_loss: 0.3668 - div_term: 1.8794\n",
      "Epoch 357/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: 0.0293 - d_loss: 0.3649 - div_term: 1.7905\n",
      "Epoch 358/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.0848 - d_loss: 0.3783 - div_term: 1.8137\n",
      "Epoch 359/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: 0.0282 - d_loss: 0.3484 - div_term: 1.8304\n",
      "Epoch 360/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: 0.1965 - d_loss: 0.3569 - div_term: 1.6306\n",
      "Epoch 361/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.3150 - d_loss: 0.3060 - div_term: 1.6479\n",
      "Epoch 362/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: 0.0811 - d_loss: 0.3514 - div_term: 1.7971\n",
      "Epoch 363/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.1112 - d_loss: 0.3646 - div_term: 1.8560\n",
      "Epoch 364/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: 0.0418 - d_loss: 0.4653 - div_term: 1.9453\n",
      "Epoch 365/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.2060 - d_loss: 0.4688 - div_term: 1.7759\n",
      "Epoch 366/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.3303 - d_loss: 0.4221 - div_term: 1.9196\n",
      "Epoch 367/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.3034 - d_loss: 0.4667 - div_term: 1.6889\n",
      "Epoch 368/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.4437 - d_loss: 0.4768 - div_term: 1.7650\n",
      "Epoch 369/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.3576 - d_loss: 0.5142 - div_term: 1.6901\n",
      "Epoch 370/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.4826 - d_loss: 0.4925 - div_term: 1.7121\n",
      "Epoch 371/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.7069 - d_loss: 0.4885 - div_term: 1.9696\n",
      "Epoch 372/420\n",
      "136/136 [==============================] - 9s 69ms/step - g_loss: -0.6243 - d_loss: 0.4996 - div_term: 1.8527\n",
      "Epoch 373/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: -0.6830 - d_loss: 0.4874 - div_term: 1.8769\n",
      "Epoch 374/420\n",
      "136/136 [==============================] - 10s 70ms/step - g_loss: -0.5287 - d_loss: 0.5297 - div_term: 1.7521\n",
      "Epoch 375/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.6262 - d_loss: 0.4875 - div_term: 1.8669\n",
      "Epoch 376/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.7510 - d_loss: 0.4908 - div_term: 1.9309\n",
      "Epoch 377/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.7890 - d_loss: 0.4742 - div_term: 1.9904\n",
      "Epoch 378/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.7757 - d_loss: 0.5016 - div_term: 1.9274\n",
      "Epoch 379/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.5804 - d_loss: 0.4793 - div_term: 1.8421\n",
      "Epoch 380/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.3680 - d_loss: 0.5923 - div_term: 1.6475\n",
      "Epoch 381/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.7734 - d_loss: 0.5597 - div_term: 1.8618\n",
      "Epoch 382/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -1.0883 - d_loss: 0.5419 - div_term: 2.0910\n",
      "Epoch 383/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -1.0369 - d_loss: 0.5352 - div_term: 2.0464\n",
      "Epoch 384/420\n",
      "136/136 [==============================] - 9s 68ms/step - g_loss: -0.8156 - d_loss: 0.5110 - div_term: 1.9239\n",
      "Epoch 385/420\n",
      "136/136 [==============================] - 9s 67ms/step - g_loss: -0.7295 - d_loss: 0.5307 - div_term: 1.8742\n",
      "Epoch 386/420\n",
      "136/136 [==============================] - 9s 70ms/step - g_loss: -1.0349 - d_loss: 0.5974 - div_term: 1.9321\n",
      "Epoch 387/420\n",
      "136/136 [==============================] - 106s 783ms/step - g_loss: -0.9619 - d_loss: 0.5738 - div_term: 1.9371\n",
      "Epoch 388/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.8950 - d_loss: 0.5480 - div_term: 1.8718\n",
      "Epoch 389/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.7058 - d_loss: 0.5263 - div_term: 1.7917\n",
      "Epoch 390/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.6961 - d_loss: 0.5052 - div_term: 1.8073\n",
      "Epoch 391/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.7596 - d_loss: 0.5146 - div_term: 1.8007\n",
      "Epoch 392/420\n",
      "136/136 [==============================] - 198s 1s/step - g_loss: -0.5316 - d_loss: 0.5062 - div_term: 1.7619\n",
      "Epoch 393/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.6132 - d_loss: 0.5220 - div_term: 1.7168\n",
      "Epoch 394/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.5970 - d_loss: 0.5199 - div_term: 1.7703\n",
      "Epoch 395/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.8084 - d_loss: 0.5484 - div_term: 1.8516\n",
      "Epoch 396/420\n",
      "136/136 [==============================] - 128s 945ms/step - g_loss: -0.5886 - d_loss: 0.5524 - div_term: 1.5888\n",
      "Epoch 397/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.5068 - d_loss: 0.5037 - div_term: 1.6825\n",
      "Epoch 398/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.7076 - d_loss: 0.5179 - div_term: 1.8357\n",
      "Epoch 399/420\n",
      "136/136 [==============================] - 1006s 7s/step - g_loss: -0.6209 - d_loss: 0.4958 - div_term: 1.8272\n",
      "Epoch 400/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.8369 - d_loss: 0.6193 - div_term: 1.9527\n",
      "Epoch 401/420\n",
      "136/136 [==============================] - 227s 2s/step - g_loss: -0.0820 - d_loss: 0.4264 - div_term: 1.5928\n",
      "Epoch 402/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.0856 - d_loss: 0.4907 - div_term: 1.8585\n",
      "Epoch 403/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.9290 - d_loss: 0.6459 - div_term: 2.0372\n",
      "Epoch 404/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -1.3315 - d_loss: 0.6797 - div_term: 2.1865\n",
      "Epoch 405/420\n",
      "136/136 [==============================] - 81s 600ms/step - g_loss: -1.2377 - d_loss: 0.6523 - div_term: 2.1964\n",
      "Epoch 406/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -1.0420 - d_loss: 0.5979 - div_term: 1.9890\n",
      "Epoch 407/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -1.0528 - d_loss: 0.5883 - div_term: 2.0117\n",
      "Epoch 408/420\n",
      "136/136 [==============================] - 60s 442ms/step - g_loss: -1.0651 - d_loss: 0.5997 - div_term: 1.9550\n",
      "Epoch 409/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.9265 - d_loss: 0.5688 - div_term: 1.9675\n",
      "Epoch 410/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -0.9921 - d_loss: 0.5824 - div_term: 1.9905\n",
      "Epoch 411/420\n",
      "136/136 [==============================] - 411s 3s/step - g_loss: -1.0957 - d_loss: 0.5837 - div_term: 2.1077\n",
      "Epoch 412/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -1.2264 - d_loss: 0.5778 - div_term: 2.1605\n",
      "Epoch 413/420\n",
      "136/136 [==============================] - 10s 73ms/step - g_loss: -1.0274 - d_loss: 0.5629 - div_term: 2.0680\n",
      "Epoch 414/420\n",
      "136/136 [==============================] - 278s 2s/step - g_loss: -0.9756 - d_loss: 0.5676 - div_term: 2.0586\n",
      "Epoch 415/420\n",
      "136/136 [==============================] - 16s 121ms/step - g_loss: -0.8831 - d_loss: 0.5437 - div_term: 1.9692\n",
      "Epoch 416/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.8661 - d_loss: 0.5912 - div_term: 1.9783\n",
      "Epoch 417/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.8122 - d_loss: 0.5290 - div_term: 1.8299\n",
      "Epoch 418/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.7692 - d_loss: 0.5770 - div_term: 1.8556\n",
      "Epoch 419/420\n",
      "136/136 [==============================] - 10s 71ms/step - g_loss: -0.8053 - d_loss: 0.5278 - div_term: 1.9680\n",
      "Epoch 420/420\n",
      "136/136 [==============================] - 10s 72ms/step - g_loss: -0.9128 - d_loss: 0.5227 - div_term: 2.0665\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/new_cgan/1/generator/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/new_cgan/1/generator/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/new_cgan/1/discriminator/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/new_cgan/1/discriminator/assets\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    cond_gan = ConditionalGAN(\n",
    "        num_features=NUM_FEATURES,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        discriminator=ConditionalGAN.conditional_discriminator(\n",
    "            hidden_units=SEQ_LENGTH, \n",
    "            seq_length=SEQ_LENGTH, \n",
    "            num_features=NUM_FEATURES,\n",
    "            filters=[32, 64, 32],\n",
    "            activation_function= ACTIVATION,\n",
    "            architecture=D_ARCHITECTURE, \n",
    "            #head_size=wandb.config.head_size#wandb.config.d_architecture\n",
    "            #filters=[wandb.config.filter1, wandb.config.filter2, wandb.config.filter3],\n",
    "            #kernel_sizes=[wandb.config.kernel_size1, wandb.config.kernel_size2, wandb.config.kernel_size3]\n",
    "            ),\n",
    "        generator=ConditionalGAN.conditional_generator(\n",
    "            hidden_units=SEQ_LENGTH, \n",
    "            seq_length=SEQ_LENGTH, \n",
    "            latent_dim=LATENT_DIM,\n",
    "            num_features=NUM_FEATURES,\n",
    "            activation_function=ACTIVATION\n",
    "        )\n",
    "    )\n",
    "    if DP_TRAINING:\n",
    "\n",
    "        mult = 1 # TODO\n",
    "        config[\"noise_multiplier\"] = noise_multipliers[mult]\n",
    "\n",
    "        d_optimizer = dp_optimizer_vectorized.VectorizedDPAdamOptimizer( #vectorized adam am schnellsten\n",
    "            l2_norm_clip=L2_NORM_CLIP,\n",
    "            noise_multiplier=noise_multipliers[mult],\n",
    "            num_microbatches=NUM_MICROBATCHES,\n",
    "            learning_rate=DP_LEARNING_RATE\n",
    "        )\n",
    "    else:\n",
    "        d_optimizer = Adam(learning_rate=LEARNING_RATE, beta_1=0.5) # get_optimizer(0.0002, wandb.config.optimizer)#\n",
    "\n",
    "    g_optimizer = Adam(learning_rate=LEARNING_RATE, beta_1=0.5) # get_optimizer(0.0002, wandb.config.optimizer)#\n",
    "\n",
    "    cond_gan.compile(\n",
    "        d_optimizer= d_optimizer, # Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "        g_optimizer= g_optimizer, # Adam(learning_rate=0.0002, beta_1=0.5), #optimizer\n",
    "        loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    )\n",
    "\n",
    "    print(f\"{cond_gan.d_optimizer} is used\")\n",
    "\n",
    "    history = cond_gan.fit(\n",
    "        dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[\n",
    "            # GANMonitor(\n",
    "            #     trainmos,\n",
    "            #     trainnomos,\n",
    "            #     testmos,\n",
    "            #     testnomos,\n",
    "            #     randomTrainMos,\n",
    "            #     randomTrainNoMos,\n",
    "            #     randomTestMos,\n",
    "            #     randomTestNoMos,\n",
    "            #     num_seq=50,\n",
    "            #     save_path=None,\n",
    "            #     batch_size=BATCH_SIZE,\n",
    "            #     seq_length=SEQ_LENGTH,\n",
    "            #     num_features=NUM_FEATURES,\n",
    "            #     dp=DP_TRAINING,\n",
    "            # )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if DP_TRAINING:\n",
    "        base_path = f\"models/new_cgan/{mult}/\"\n",
    "        cond_gan.generator.save(f\"{base_path}generator\")\n",
    "        cond_gan.discriminator.save(f\"{base_path}discriminator\")\n",
    "    elif LOSO_TRAINING_WITHOUT_SUBJECT:\n",
    "        base_path = f\"models/no_dp/loso/sub{LOSO_TRAINING_WITHOUT_SUBJECT}/{wandb.run.name}/\"\n",
    "        cond_gan.generator.save(f\"{base_path}cgan_generator\")\n",
    "        cond_gan.discriminator.save(f\"{base_path}cgan_discriminator\")\n",
    "    else:\n",
    "        base_path = f\"models/no_dp/{wandb.run.name}/\"\n",
    "        cond_gan.generator.save(f\"{base_path}cgan_generator\")\n",
    "        cond_gan.discriminator.save(f\"{base_path}cgan_discriminator\")\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"say 'C GAN IST FERTIG'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
